---
title: 图神经网络论文阅读
date: 2025-03-02 18:51:25
tags:
  - 机器学习
  - 数据挖掘
categories:
  - 图神经网络研究
---

# Graph Attention Networks

### 1. 研究背景与动机

- **图数据的挑战**：传统的卷积神经网络（CNNs）适用于规则网格数据（如图像），但很多实际任务（如社交网络、生物网络等）中的数据以图的形式存在，结构不规则。
- 现有方法的不足：
  - **谱方法**：基于图拉普拉斯特征分解，**计算量大**且**依赖于特定图结构**，难以泛化到新图。
  - **非谱方法**（MoNet, GraphSAGE)：直接在图上定义卷积操作，但难以统一处理邻居数目不固定的问题。
- **自注意力机制启发**：近年来，自注意力在序列模型（如机器翻译）中表现突出，启发作者将自注意力思想应用到图数据中，更灵活地聚合邻居信息，以完成图数据的节点分类任务。
- 自注意力结构的特点：
  1. 计算高效，所有运算可以并行化
  2. 通过指定任意的邻节点权重，可以处理节点度数不同的图
  3. 模型可直接应用于归纳学习任务，可以泛化到模型从未见过的图

------

### 2. Graph Attention Networks（GAT）方法简介

- **核心思想**：通过自注意力机制，让每个节点在聚合自身邻居特征时，根据邻居的重要性分配不同权重，从而获得更有效的节点表示。
- **输入与输出**：输入为节点特征集合 **h**，输出为更新后的节点特征 **h**′，支持特征维度变换。
- **Graph Attention Layer的关键步骤**：
  1. **线性变换**：通过权重矩阵 **W** 对所有节点的输入特征进行同样的线性变换，得到新的特征表示。
  2. **计算注意力系数**：
     - 对于每个节点 i 及其一阶邻居 j（含i），利用一个共享的注意力机制（单层前馈网络加上 LeakyReLU 激活）计算原始的注意力分数 $e_{ij}$。
     - 对每个节点的所有邻居分数进行 softmax 归一化，得到归一化的注意力系数 $\alpha_{ij}$。
  3. **特征聚合**：使用归一化后的注意力系数，对邻居节点的特征进行加权求和，并经过sigmoid非线性激活，生成新的节点表示。
  4. **多头注意力**：通过并行执行多个独立的注意力机制（即多头注意力），提升模型的稳定性与表达能力。最后在输出层可选择拼接（中间层）或平均（输出层）多头结果。
- **优势**：
  - **计算高效**：无需矩阵分解或全局图结构信息，计算复杂度与GCN相当（O(∣V∣FF′+∣E∣F′) ，支持并行化。
  - **动态权重分配**：通过注意力机制为不同邻居分配不同权重，提升模型表达能力。
  - **灵活性强**：不依赖于图的全局结构，可处理有向图、不同大小的邻居集合，且适用于归纳学习（处理训练时未见过的新图）。
  - **可解释性**：注意力系数反映了模型在聚合邻居信息时对各邻居的关注程度，有助于理解模型的决策过程。

------

### 3. 实验设置与结果

- **数据集**：
  - **转导任务**：使用 Cora、Citeseer 和 Pubmed 三个标准引文网络数据集，节点特征为文档的词袋表示，每个节点对应一个类别。
  - **归纳任务**：使用蛋白质相互作用（PPI）数据集，该数据集包含多个不同的图，测试时的图完全未在训练中出现。
- **模型结构**：
  - **转导任务**：采用两层 GAT 模型，第一层使用多头注意力（例如 8 个头）提取中间特征，第二层用于分类，配合 dropout 和 L2 正则化。
  - **归纳任务**：采用三层 GAT 模型，并利用跳跃连接来缓解深层模型训练中的信息衰减问题。
- **实验结果**：
  - 在转导任务上，GAT 在 Cora、Citeseer 和 Pubmed 数据集上均超过或匹配了如 GCN、Chebyshev 滤波器和 MoNet 等多种方法的表现。
  - 在归纳任务（PPI 数据集）上，GAT 显著提升了微平均 F1 得分，验证了其在新图环境下的强泛化能力，同时也证明了赋予邻居不同权重的重要性。

------

### 4. 结论与未来工作

- **主要贡献**：
  - 提出了基于自注意力机制的图神经网络架构，能够灵活高效地处理图结构数据。
  - 在不需要事先知道全局图结构的前提下，实现了对不同大小邻居集合的有效处理，并适用于归纳学习。
  - 在多个标准数据集上达到了最先进的性能，验证了模型的有效性和广泛适用性。
- **未来研究方向**：
  - 扩展模型到图分类任务。
  - 探索如何更高效地处理大规模图数据和提高批处理能力。
  - 深入利用注意力机制进行模型解释，并尝试整合边特征信息以处理更复杂的图数据。

# Deep Graph Infomax (DGI)

### 1. 研究背景与动机

- **图数据的挑战**：传统的图神经网络（如GCN）多采用监督学习，而在许多实际场景下图数据往往没有标注信息。
- **无监督学习的需求**：现有方法常基于随机游走或邻近重构目标，但这类方法可能过分强调局部邻接信息，忽略了全局结构特征。
- **核心思想**：如何在不依赖标签的情况下，从图结构数据中学习到既保留局部细节又能反映全局结构的节点表示。

------

### 2. 方法概述：Deep Graph Infomax (DGI)

- **局部与全局互信息最大化**：DGI 的主要思想是利用互信息最大化，通过对比图中局部（即节点及其邻域）表示与图整体全局摘要之间的关系，促使模型学习到富有全局结构信息的节点表示。
- 方法流程：
  1. **编码器（Encoder）**：采用图卷积网络（GCN）或其它图神经网络模型，从输入的节点特征和邻接矩阵中提取局部“补丁”表示。
  2. **读取函数（Readout Function）**：将所有节点的局部表示聚合成一个全局摘要向量，反映整个图的全局信息。
  3. **判别器（Discriminator）**：通过一个二分类器对比真实的局部-全局配对与负样本配对（通过对输入数据进行随机扰动或负采样获得），采用二元交叉熵损失函数进行训练，进而最大化局部表示与全局摘要之间的互信息。

------

### 3. 理论分析

- **理论证明**：论文提供了一系列引理和定理，证明了在局部与全局表示间最小化判别器的分类错误能够上界地最大化这两者之间的互信息。
- **互信息与最优判别器**：当读取函数具备单射性（injective）时，全局摘要能够捕获输入图的所有信息，进而使得模型在理论上达到最优互信息。

------

### 4. 实验与结果

- 任务设置：
  - **转导式任务**：在 Cora、Citeseer 和 Pubmed 等引用网络上进行节点分类实验。
  - **归纳式任务**：在 Reddit 社交网络和 PPI 蛋白质相互作用网络上进行归纳式节点分类实验。
- 结果表现：
  - 在转导式任务上，DGI 的表现超越了许多无监督方法，并在某些数据集上甚至与监督学习的 GCN 模型竞争。
  - 在归纳式任务中，DGI 同样展示了优越的性能，证明了其在大规模和多图场景下的适用性。
- **鲁棒性**：DGI 对负样本生成（如特征打乱或邻接矩阵扰动）的策略具有较高的鲁棒性，能够在不同负采样方式下保持良好性能。

------

### 5. 定性分析与结论

- **可视化分析**：通过 t-SNE 等技术对节点表示进行可视化，结果显示经过 DGI 学习的表示在低维空间中形成了明显的类别聚类结构。
- **嵌入维度分析**：研究发现，部分嵌入维度在区分正负样本时起到关键作用，而其余维度则主要编码了有用的节点特征信息；即便移除部分维度，分类性能仍能保持较高水平。
- **总结**：DGI 提出了一种全新的图无监督学习方法，通过局部与全局信息的对比，有效捕捉了图数据的结构特征，并在多个标准数据集上验证了其优异的性能，为无监督图表示学习提供了新的思路和方向。

------

整体而言，DGI 的创新点在于利用互信息最大化来桥接局部节点表示与全局图信息之间的联系，从而在无监督的情形下学习到高质量的节点表示，为后续的图任务（如节点分类、社区检测等）提供了坚实的基础。

# Semi-Supervised Classification with Graph Convolutional Networks

### 1. **研究背景与动机**

 本文针对图结构数据中的节点分类问题，提出了一种半监督学习方法。传统方法往往依赖于显式的图拉普拉斯正则化，但这在实际应用中可能会受到图中边仅表示节点相似性的限制。作者希望利用图结构和节点特征的联合信息，通过神经网络直接对图进行建模，从而提高分类效果。

### 2. **方法原理与模型构建**

- **谱图卷积近似**：论文首先从谱图卷积出发，利用切比雪夫多项式对卷积操作进行近似，从而实现局部化滤波。通过限制多项式的阶数，最终得到一阶近似形式。

- **层级传播规则**：进一步简化后，提出了基于重新归一化技巧的传播规则，即

  H(l+1) = σ( D̃^(–1/2) Ã D̃^(–1/2) H(l) W(l) )

  其中 Ã = A + Iₙ，D̃ 是 Ã 的度矩阵，这样设计既考虑了节点自连接，又解决了深层网络中可能出现的数值不稳定问题。

### 3. **半监督节点分类框架**

- **网络架构**：以两层GCN为例，第一层将输入特征与图结构信息融合后生成隐藏表示，第二层再通过 softmax 输出各节点的分类概率。
- **损失函数**：采用交叉熵损失，仅在少量标记节点上进行监督训练，从而利用整个图结构信息进行传播和特征提取。
- **训练方式**：由于数据集（如引用网络）的规模适中，可采用全批量梯度下降，并通过 dropout 等技术防止过拟合。 

### 4. **实验与结果**

- 在 Citeseer、Cora、Pubmed 以及知识图谱 NELL 数据集上进行实验，结果显示该模型在分类准确率和计算效率上均明显优于传统方法，如标签传播、深度嵌入、Planetoid 等。
- 另外，实验还分析了不同层数和传播模型对性能的影响，证明了重新归一化传播规则在性能和稳定性上的优势。 

### 5. **讨论与未来工作**

- 讨论部分分析了GCN在捕捉局部结构信息方面的优势，并指出模型在处理大规模图数据时内存消耗较大，未来可以探索基于小批量随机梯度下降的扩展方法。
- 论文还探讨了与 Weisfeiler-Lehman 算法的关系，说明即使是未经过训练的GCN也能作为强大的特征提取器。 

### 6. **结论**

文章提出的GCN模型通过简化谱卷积操作，构建了高效且易扩展的半监督分类框架，成功地将图结构信息与节点特征结合起来，不仅提升了分类准确率，同时显著降低了计算复杂度，为后续图神经网络的发展奠定了基础。

